{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown.categories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories=['adventure', 'editorial', 'news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt Cookie Manager: \"Don ...\n",
      "grail.txt SCENE 1: [wind] [clo ...\n",
      "overheard.txt White guy: So, do yo ...\n",
      "pirates.txt PIRATES OF THE CARRI ...\n",
      "singles.txt 25 SEXY MALE, seeks  ...\n",
      "wine.txt Lovely delicate, fra ...\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import field\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "for fileid in webtext.fileids() :\n",
    "    print(fileid, webtext.raw(fileid)[:20], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.', 'It was developed with a focus on enabling fast experimentation.', 'Being able to go from idea to result as fast as possible is key to doing good research.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ex = \"Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.\"\n",
    "\n",
    "sent_tokens = sent_tokenize(ex)\n",
    "\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keras',\n",
       " 'is',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'API',\n",
       " 'written',\n",
       " 'in',\n",
       " 'Python',\n",
       " ',',\n",
       " 'running',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'platform',\n",
       " 'TensorFlow',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'developed',\n",
       " 'with',\n",
       " 'a',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'enabling',\n",
       " 'fast',\n",
       " 'experimentation',\n",
       " '.',\n",
       " 'Being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'go',\n",
       " 'from',\n",
       " 'idea',\n",
       " 'to',\n",
       " 'result',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'possible',\n",
       " 'is',\n",
       " 'key',\n",
       " 'to',\n",
       " 'doing',\n",
       " 'good',\n",
       " 'research',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(ex)\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Keras', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('deep', 'JJ'),\n",
       " ('learning', 'NN'),\n",
       " ('API', 'NNP'),\n",
       " ('written', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Python', 'NNP'),\n",
       " (',', ','),\n",
       " ('running', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('top', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('platform', 'NN'),\n",
       " ('TensorFlow', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('developed', 'VBN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('focus', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('enabling', 'VBG'),\n",
       " ('fast', 'JJ'),\n",
       " ('experimentation', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Being', 'VBG'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('go', 'VB'),\n",
       " ('from', 'IN'),\n",
       " ('idea', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('result', 'VB'),\n",
       " ('as', 'RB'),\n",
       " ('fast', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('possible', 'JJ'),\n",
       " ('is', 'VBZ'),\n",
       " ('key', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('doing', 'VBG'),\n",
       " ('good', 'JJ'),\n",
       " ('research', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "tags = pos_tag(word_tokens)\n",
    "\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TO',\n",
       " 'VBN',\n",
       " 'VBZ',\n",
       " 'VBG',\n",
       " 'DT',\n",
       " '.',\n",
       " 'NN',\n",
       " 'PRP',\n",
       " 'IN',\n",
       " ',',\n",
       " 'VBD',\n",
       " 'VB',\n",
       " 'NNP',\n",
       " 'JJ',\n",
       " 'RB']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tag = []\n",
    "\n",
    "for pair in tags :\n",
    "    list_of_tag.append(pair[1])\n",
    "\n",
    "\n",
    "list_of_tag = list(set(list_of_tag))\n",
    "\n",
    "list_of_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "None\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "None\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "None\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "None\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "None\n",
      ",: comma\n",
      "    ,\n",
      "None\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "None\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "None\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "None\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for pos in list_of_tag :\n",
    "    print(nltk.help.upenn_tagset(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"Keras is a deep learning API written in Python, \n",
    "running on top of the machine learning platform TensorFlow. \n",
    "It was developed with a focus on enabling fast experimentation. \n",
    "Being able to go from idea to result as fast as possible is key to doing good research.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Keras', 'is', 'a', 'deep', 'learning', 'API', 'written', 'in', 'Python', ',', 'running', 'on', 'top', 'of', 'the', 'machine', 'learning', 'platform', 'TensorFlow', '.', 'It', 'was', 'developed', 'with', 'a', 'focus', 'on', 'enabling', 'fast', 'experimentation', '.', 'Being', 'able', 'to', 'go', 'from', 'idea', 'to', 'result', 'as', 'fast', 'as', 'possible', 'is', 'key', 'to', 'doing', 'good', 'research', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "Lancaster = nltk.LancasterStemmer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kera',\n",
       " 'is',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'learn',\n",
       " 'api',\n",
       " 'written',\n",
       " 'in',\n",
       " 'python',\n",
       " ',',\n",
       " 'run',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'the',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'platform',\n",
       " 'tensorflow',\n",
       " '.',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'develop',\n",
       " 'with',\n",
       " 'a',\n",
       " 'focu',\n",
       " 'on',\n",
       " 'enabl',\n",
       " 'fast',\n",
       " 'experiment',\n",
       " '.',\n",
       " 'be',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'go',\n",
       " 'from',\n",
       " 'idea',\n",
       " 'to',\n",
       " 'result',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'possibl',\n",
       " 'is',\n",
       " 'key',\n",
       " 'to',\n",
       " 'do',\n",
       " 'good',\n",
       " 'research',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kera',\n",
       " 'is',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'learn',\n",
       " 'ap',\n",
       " 'writ',\n",
       " 'in',\n",
       " 'python',\n",
       " ',',\n",
       " 'run',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'the',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'platform',\n",
       " 'tensorflow',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'develop',\n",
       " 'with',\n",
       " 'a',\n",
       " 'foc',\n",
       " 'on',\n",
       " 'en',\n",
       " 'fast',\n",
       " 'expery',\n",
       " '.',\n",
       " 'being',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'go',\n",
       " 'from',\n",
       " 'ide',\n",
       " 'to',\n",
       " 'result',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'poss',\n",
       " 'is',\n",
       " 'key',\n",
       " 'to',\n",
       " 'doing',\n",
       " 'good',\n",
       " 'research',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Lancaster.stem(t) for t in tokens ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "wnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = \"The big yellow bird flew over the house \"\n",
    "\n",
    "def preprocess(sent) :\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('yellow', 'JJ'),\n",
       " ('bird', 'NN'),\n",
       " ('flew', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('house', 'NN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = preprocess(ex)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT big/JJ yellow/JJ bird/NN)\n",
      "  flew/VBD\n",
      "  over/IN\n",
      "  (NP the/DT house/NN))\n"
     ]
    }
   ],
   "source": [
    "np_rule = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "cp = nltk.RegexpParser(np_rule)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdb51b583fe5a79700d1bb3df041392ce830c189fede056fa7ac4656b08f5336"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
